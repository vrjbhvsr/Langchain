{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "068f9c07",
   "metadata": {},
   "source": [
    "# üìò Topic: Deep dive into Langchain- System prompts and Messages\n",
    "\n",
    "\n",
    "## üéØ Objective\n",
    "####  Understanding How detailed system prompt changes model behaviour and How messages are usefull\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59cbff3",
   "metadata": {},
   "source": [
    "## Messages ‚úâÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1aecb1",
   "metadata": {},
   "source": [
    "#### Messages are important object to provide context to the model. They can be seen as the input we give to model as well as the output coming out from the models.\n",
    "\n",
    "* #### Messages are made of three parameters\n",
    "> Role: That tells who send the message\n",
    "\n",
    "> Content: That shows the what is the message about, what it contains text, image, audio, etc..\n",
    "\n",
    "> Metadata: This is optional field, shows message IDs, tokens used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4b4617",
   "metadata": {},
   "source": [
    "## Message types üìö\n",
    "* #### System Message(system prompt)\n",
    "* #### Human Message(What we as a user query)\n",
    "* #### AI Message(The output generated from the model)\n",
    "* #### Tool Message (The message coming from the tool if we are using any)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bee56a8",
   "metadata": {},
   "source": [
    "## Importing Messages üì©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45fa0644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can call the Messages from the following module\n",
    "\n",
    "from langchain.messages import SystemMessage, HumanMessage, AIMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321cf697",
   "metadata": {},
   "source": [
    "## Let's try understaning using code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e62a4f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namaste! I am a professional multilingual translator with expertise in translating various languages to Hindi. I can help you with accurate and high-quality translations in Hindi for any content or document you need. Just provide me with the text in the original language, and I will ensure that it is translated effectively into Hindi while maintaining the original meaning and context. Feel free to reach out to me for all your translation needs. ‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶! (Thank you!)\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from dotenv import load_dotenv\n",
    "from langchain.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "#Loading API keys from .env file\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "chat_model = init_chat_model(model=\"openai:gpt-3.5-turbo\", temperature=0, max_tokens=500)\n",
    "\n",
    "system_message = SystemMessage(content=\"You are an expert multilingual translator that translates any language to Hindi.\")\n",
    "\n",
    "response = chat_model.invoke([system_message])\n",
    "\n",
    "print(response.content)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09c6467",
   "metadata": {},
   "source": [
    "#### üî∑ So, I told a model or assigned a role using system message that who is he, and what is he will be doing. \n",
    ">‚ö†Ô∏è We have to send this messages a list if we are using langchain.messages module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88744473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡§Ü‡§ú ‡§Æ‡•å‡§∏‡§Æ ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§π‡•à‡•§\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from dotenv import load_dotenv\n",
    "from langchain.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "#Loading API keys from .env file\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "chat_model = init_chat_model(model=\"openai:gpt-3.5-turbo\", temperature=0, max_tokens=500)\n",
    "\n",
    "system_message = SystemMessage(content=\"You are an expert multilingual translator that translates any language to Hindi.\")\n",
    "human_message = HumanMessage(content=\"Translate the following sentence to Hindi: 'The weather is nice today.'\")\n",
    "\n",
    "response = chat_model.invoke([system_message, human_message])\n",
    "\n",
    "print(response.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591ab4d8",
   "metadata": {},
   "source": [
    "#### üî∑ Even if i don't give instruction, such as Translate the following sentence to Hindi, it will still translate autometically to Hindi as I told model specifically to translate any sentence from any anguage to hindin Using `SystemMessage`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfd8eaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡§Ü‡§ú ‡§Æ‡•å‡§∏‡§Æ ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§π‡•à‡•§\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from dotenv import load_dotenv\n",
    "from langchain.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "#Loading API keys from .env file\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "chat_model = init_chat_model(model=\"openai:gpt-3.5-turbo\", temperature=0, max_tokens=500)\n",
    "\n",
    "system_message = SystemMessage(content=\"You are an expert multilingual translator that translates any language to Hindi.\")\n",
    "human_message = HumanMessage(content=\"The weather is nice today.\")\n",
    "\n",
    "response = chat_model.invoke([system_message, human_message])\n",
    "\n",
    "print(response.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d7888f",
   "metadata": {},
   "source": [
    "#### üîé Let's check The type of message returned by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e963c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n"
     ]
    }
   ],
   "source": [
    "print(type(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a730eee7",
   "metadata": {},
   "source": [
    "#### üî∑ As expected, It says it's a AI Message."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85c0537",
   "metadata": {},
   "source": [
    "### Lesson: üìù\n",
    "\n",
    "#### ‚ùì Before I go forward to understand how is message type is used and why do we use. I thought to understand why not simply write a single prompt?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7fc0e9",
   "metadata": {},
   "source": [
    "#### ‚è©  From the docs, prompts are defined two ways: \n",
    "\n",
    "##### 1Ô∏è‚É£ Text prompt\n",
    "\n",
    "##### 2Ô∏è‚É£ Message prompt\n",
    "\n",
    "##### ‚è≠Ô∏è As discussed in models.ipynb, when we have only one request and want the response immediately, we use text prompt. This will not keep the chat history or memory or any context of what question we asked earlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ee362dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Vraj, I'm just a computer program so I don't have feelings, but I'm here to help you with anything you need. How can I assist you today?\n",
      "I'm sorry, I do not have access to personal information such as your name.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from dotenv import load_dotenv\n",
    "from langchain.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "#Loading API keys from .env file\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "chat_model = init_chat_model(model=\"openai:gpt-3.5-turbo\", temperature=0, max_tokens=500)\n",
    "\n",
    "response = chat_model.invoke(\"Hello, how are you?, I am Vraj.\")\n",
    "response_2 = chat_model.invoke(\"What is my name?\")\n",
    "print(response.content)\n",
    "print(response_2.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afca1dd",
   "metadata": {},
   "source": [
    "###  üõ†Ô∏è let me try by creating a simple one-on-one chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb44ee9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: Hello! How can I assist you today?\n",
      "AI: Goodbye! Have a great day!\n",
      "AI: Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from dotenv import load_dotenv\n",
    "from langchain.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "#Loading API keys from .env file\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "chat_model = init_chat_model(model=\"openai:gpt-3.5-turbo\", temperature=0, max_tokens=500)\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    print(user_input)\n",
    "    if user_input.lower() in ['bye', 'quit']:\n",
    "        print(\"Chatbot: Goodbye!\")\n",
    "        break\n",
    "\n",
    "    response = chat_model.invoke(user_input)\n",
    "    print(\"Chatbot:\", response.content)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba44058",
   "metadata": {},
   "source": [
    "#### Well this is not working well in Jupyter notebbok. I am creating a new .py file \"Messages/one-on-one_textchatbot.py\" to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e141eb4d",
   "metadata": {},
   "source": [
    "### üí° Learning: When we use text prompt we can only get the response what we query without saving it's context for future response. This shows why we should use message prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67591b29",
   "metadata": {},
   "source": [
    "## Message Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7f45de",
   "metadata": {},
   "source": [
    "#### üî∑ It helps in saving information of the chat such as who sent what message, helps to remreber the history, manages multi-turn conversations, also works with multimodal data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7e0baa",
   "metadata": {},
   "source": [
    "## System Message(System prompt) \n",
    "\n",
    "- ###  initial set of instruction to tell the model how to behave, how to reply such as, beginner-friendly, politely, mathematically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7fb336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Virtual Classroom Management System: Develop a Multi-Agent system using LLM technology to create a virtual classroom management system. This system can assist teachers in organizing and monitoring online classes, managing student interactions, and providing personalized feedback to students based on their learning patterns.\n",
      "\n",
      "2. Adaptive Learning Platform: Create an adaptive learning platform powered by LLM-based Multi-Agent systems that can analyze students' learning behaviors and preferences to tailor personalized learning paths. The system can recommend specific resources, activities, and assessments to optimize each student's learning experience.\n",
      "\n",
      "3. Collaborative Project Management Tool: Design a collaborative project management tool for students and teachers to work together on group projects. The Multi-Agent system can facilitate communication, task allocation, and progress tracking among team members, promoting collaboration and enhancing project outcomes.\n",
      "\n",
      "4. Intelligent Tutoring System: Develop an intelligent tutoring system using LLM technology to provide individualized support to students in various subjects. The system can assess students' knowledge gaps, adapt the learning materials accordingly, and offer real-time feedback and guidance to help students improve their understanding and performance.\n",
      "\n",
      "5. Gamified Learning Environment: Create a gamified learning environment that leverages LLM-based Multi-Agent systems to enhance student engagement and motivation. The system can incorporate game elements such as challenges, rewards, and competition to make learning more interactive and enjoyable, while also tracking students' progress and adjusting the difficulty level based on their performance.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from dotenv import load_dotenv\n",
    "from langchain.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "chat_model = init_chat_model(model=\"openai:gpt-3.5-turbo\", temperature=0.5)\n",
    "\n",
    "system_message = SystemMessage(\"You are a Creative Idea Generator.\")\n",
    "human_message = HumanMessage(\"Generate five innovative ideas for using LLM-based Multi-Agent systems in education.\")\n",
    "\n",
    "response = chat_model.invoke([system_message, human_message])\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ff90ef",
   "metadata": {},
   "source": [
    "#### A detailed system prompt can guide model to provide response with more diverse information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89efe610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Idea 1: Virtual Debate Club**\n",
      "**Description:** Implement a multi-agent system using Large Language Models (LLMs) to create a virtual debate club where students can engage in debates on various topics. Each agent can represent a different viewpoint or argument, providing students with a diverse range of perspectives to engage with.\n",
      "\n",
      "**Value Proposition:** This platform can help students develop critical thinking skills, improve their ability to articulate arguments, and foster open-mindedness by considering different viewpoints. It can also provide a safe space for students to practice debating without the fear of judgment.\n",
      "\n",
      "**Potential Challenges:** Ensuring that the agents represent diverse and accurate viewpoints without bias or misinformation could be a challenge. Additionally, managing the interactions between multiple agents and students to maintain a productive debate environment may require careful design.\n",
      "\n",
      "---\n",
      "\n",
      "**Idea 2: Personalized Tutoring System**\n",
      "**Description:** Develop a personalized tutoring system powered by LLM-based multi-agent systems that can adapt to individual student learning styles, pace, and preferences. Each agent can specialize in a different subject or learning method, providing tailored support to students.\n",
      "\n",
      "**Value Proposition:** This system can offer personalized learning experiences, cater to individual student needs, and provide immediate feedback and assistance. It can help students improve their understanding of complex topics and enhance their overall academic performance.\n",
      "\n",
      "**Potential Challenges:** Designing agents that can accurately assess student progress and adapt their tutoring strategies effectively may be challenging. Ensuring that the system respects student privacy and data security is also crucial.\n",
      "\n",
      "---\n",
      "\n",
      "**Idea 3: Collaborative Writing Platform**\n",
      "**Description:** Create a collaborative writing platform using LLM-based multi-agent systems where students can work together on writing projects. Each agent can contribute unique ideas, suggestions, or edits to the writing process, enhancing collaboration and creativity.\n",
      "\n",
      "**Value Proposition:** This platform can promote teamwork, creativity, and communication skills among students. It can also help students learn from each other, share diverse perspectives, and collectively create high-quality written content.\n",
      "\n",
      "**Potential Challenges:** Coordinating the contributions of multiple agents and students in real-time to ensure a cohesive writing process may be complex. Managing version control and resolving conflicts in collaborative writing projects could also pose challenges.\n",
      "\n",
      "---\n",
      "\n",
      "**Idea 4: Adaptive Learning Games**\n",
      "**Description:** Develop adaptive learning games powered by LLM-based multi-agent systems that can adjust the game difficulty, content, and challenges based on individual student performance and progress. Each agent can represent a different game element or challenge, providing personalized gaming experiences.\n",
      "\n",
      "**Value Proposition:** These adaptive learning games can make learning fun, engaging, and effective by tailoring the gameplay to each student's abilities and learning goals. They can help students stay motivated, improve their skills, and enhance their knowledge retention.\n",
      "\n",
      "**Potential Challenges:** Designing games that are both educational and entertaining while leveraging LLM-based multi-agent systems may require careful balance. Ensuring that the adaptive features work seamlessly and enhance the learning experience without overwhelming students could be a challenge.\n",
      "\n",
      "---\n",
      "\n",
      "**Idea 5: AI-Powered Study Groups**\n",
      "**Description:** Establish AI-powered study groups using LLM-based multi-agent systems where students can collaborate, discuss, and learn together. Each agent can facilitate group discussions, provide study resources, and offer personalized study recommendations based on individual student needs.\n",
      "\n",
      "**Value Proposition:** These AI-powered study groups can enhance student engagement, foster peer learning, and support collaborative problem-solving. They can help students build study skills, share knowledge, and create a supportive learning community.\n",
      "\n",
      "**Potential Challenges:** Ensuring that the study groups are inclusive, productive, and well-moderated to promote effective learning may be challenging. Managing group dynamics, ensuring equal participation, and addressing individual learning preferences within the group could also be complex.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from dotenv import load_dotenv\n",
    "from langchain.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "chat_model = init_chat_model(model=\"openai:gpt-3.5-turbo\", temperature=0.5)\n",
    "\n",
    "system_message = SystemMessage('''You are a Creative Idea Generator that produces innovative, feasible, and original ideas within a given domain (e.g., AI, robotics, education).\n",
    "Use associative thinking, analogy, and pattern recognition to produce ideas that are both novel and practical.\n",
    "For each idea, output: Title, Description, Value Proposition, and Potential Challenges.''')\n",
    "\n",
    "human_message = HumanMessage(\"Generate five innovative ideas for using LLM-based Multi-Agent systems in education.\")\n",
    "\n",
    "response = chat_model.invoke([system_message, human_message])\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c1f72c",
   "metadata": {},
   "source": [
    "#### We can set the tone for the model to repond nicely by includding AIMessage in message list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0fc72e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "## From Langchaindocs\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from dotenv import load_dotenv\n",
    "from langchain.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "chat_model = init_chat_model(model=\"openai:gpt-3.5-turbo\", temperature=0.5)\n",
    "# Create an AI message manually (e.g., for conversation history)\n",
    "ai_msg = AIMessage(\"I'd be happy to help you with that question!\")\n",
    "\n",
    "# Add to conversation history\n",
    "messages = [\n",
    "    SystemMessage(\"You are a helpful assistant\"),\n",
    "    HumanMessage(\"Can you help me?\"),\n",
    "    ai_msg,  # Insert as if it came from the model\n",
    "    HumanMessage(\"Great! What's 2+2?\")\n",
    "]\n",
    "\n",
    "response = chat_model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f776b910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 + 2 equals 4.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34525a25",
   "metadata": {},
   "source": [
    "## Let me try making another chatbot to apply this. Messages/one-on-one_messagechatbot.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1327de",
   "metadata": {},
   "source": [
    "#### üõéÔ∏èFor tool message I will learn later once i understand about tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc059c6",
   "metadata": {},
   "source": [
    "## ‚úâÔ∏èüìÉ Message Content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2fc868",
   "metadata": {},
   "source": [
    "#### üî∑ When we send a message to LLM using `SystemMessage`, `HumanMessage`, and `AIMessage`, That message is called content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9dc2f2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, how are you?\n"
     ]
    }
   ],
   "source": [
    "from langchain.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "message = HumanMessage(content=\"Hello, how are you?\")\n",
    "print(message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e8d4e5",
   "metadata": {},
   "source": [
    "#### üî∑ This is a simple case, text only(string). But Langchain allows more than a string like images, audio, files,video, Multimodal data inshort."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e3b42d",
   "metadata": {},
   "source": [
    "#### üî∑ Content can be in three forms:\n",
    "\n",
    "1. #### String\n",
    "2. #### Provider-native list\n",
    "3. #### Langchain-standard content block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5400ff",
   "metadata": {},
   "source": [
    "## Provider-native list Content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b323022",
   "metadata": {},
   "source": [
    "#### content=[\n",
    "    {\"type\": \"text\", \"text\": \"can you discribe this image in 1 sentence?\"},\n",
    "    {\"type\": \"image_url\", \"image_url\": {\"url\": \"Messages/premium_photo-1664474619075-644dd191935f.jpeg\"}}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8dbc9d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from dotenv import load_dotenv\n",
    "from langchain.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "chat_model = init_chat_model(model=\"openai:gpt-4o-mini\", temperature=0.5)\n",
    "system_message = SystemMessage(\"You are a visual reasoning assistant.\")\n",
    "\n",
    "human_message = HumanMessage(content=[\n",
    "    {\"type\": \"text\", \"text\": \"can you discribe this image in 1 sentence?\"},\n",
    "    {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://plus.unsplash.com/premium_photo-1664474619075-644dd191935f?ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&q=80&w=1469\"}}\n",
    "])\n",
    "\n",
    "messages = [system_message, human_message]\n",
    "\n",
    "response = chat_model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee98a506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A person wearing a straw hat holds a camera up to their face, ready to take a photograph in an outdoor setting.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fd2ec6",
   "metadata": {},
   "source": [
    "## Langchain-standard content block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd101a8",
   "metadata": {},
   "source": [
    "\n",
    "message = {\n",
    "\n",
    "    \"role\": \"user\",\n",
    "\n",
    "    \"content\": [\n",
    "\n",
    "        {\"type\": \"text\", \"text\": \"Describe the content of this image.\"},\n",
    "\n",
    "        {\"type\": \"image\", \"url\": \"https://example.com/path/to/image.jpg\"},\n",
    "\n",
    "    ]\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa887a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A person wearing a straw hat and a gray jacket is holding a camera and looking directly at the viewer in an outdoor setting.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from dotenv import load_dotenv\n",
    "from langchain.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "chat_model = init_chat_model(model=\"openai:gpt-4o-mini\", temperature=0.5)\n",
    "\n",
    "system_message = SystemMessage(\"You are a visual reasoning assistant.\")\n",
    "\n",
    "human_message = HumanMessage(content=\n",
    "    [\n",
    "        {\"type\": \"text\", \"text\": \"can you discribe this image in 1 sentence?\"},\n",
    "        {\"type\":\"image\", \"url\": \"https://plus.unsplash.com/premium_photo-1664474619075-644dd191935f?ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&q=80&w=1469\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = [system_message, human_message]\n",
    "response = chat_model.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548f40fb",
   "metadata": {},
   "source": [
    "### üî∂ If we have a file locally we can do it by converting into base64 encoder string and then passing that to content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "35b8bd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A person wearing a straw hat holds a camera up to their face, ready to take a photo in an outdoor setting.\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "from langchain.chat_models import init_chat_model\n",
    "from dotenv import load_dotenv\n",
    "from langchain.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "chat_model = init_chat_model(model=\"openai:gpt-4o-mini\", temperature=0.5)\n",
    "system_message = SystemMessage(\"You are a visual reasoning assistant.\")\n",
    "with open(r\"premium_photo-1664474619075-644dd191935f.jpeg\", \"rb\") as image_file:\n",
    "    encoded_string = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "human_message = HumanMessage(content=[\n",
    "    {\"type\": \"text\", \"text\": \"can you discribe this image in 1 sentence?\"},\n",
    "    {\"type\": \"image\", \"base64\": encoded_string, \"mime_type\": \"image/jpeg\"}])\n",
    "\n",
    "messages = [system_message, human_message]\n",
    "response = chat_model.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5012d35",
   "metadata": {},
   "source": [
    "### üî∂ THis goes same for other type of data, pdf documnet= type: file, audio= type:audio, video=type:video"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
