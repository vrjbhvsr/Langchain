{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "068f9c07",
   "metadata": {},
   "source": [
    "# üìò Topic: Deep dive into Langchain- System prompts and Messages\n",
    "\n",
    "\n",
    "## üéØ Objective\n",
    "####  Understanding How detailed system prompt changes model behaviour and How messages are usefull\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59cbff3",
   "metadata": {},
   "source": [
    "## Messages ‚úâÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1aecb1",
   "metadata": {},
   "source": [
    "#### Messages are important object to provide context to the model. They can be seen as the input we give to model as well as the output coming out from the models.\n",
    "\n",
    "* #### Messages are made of three parameters\n",
    "> Role: That tells who send the message\n",
    "\n",
    "> Content: That shows the what is the message about, what it contains text, image, audio, etc..\n",
    "\n",
    "> Metadata: This is optional field, shows message IDs, tokens used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4b4617",
   "metadata": {},
   "source": [
    "## Message types üìö\n",
    "* #### System Message(system prompt)\n",
    "* #### Human Message(What we as a user query)\n",
    "* #### AI Message(The output generated from the model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bee56a8",
   "metadata": {},
   "source": [
    "## Importing Messages üì©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45fa0644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can call the Messages from the following module\n",
    "\n",
    "from langchain.messages import SystemMessage, HumanMessage, AIMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321cf697",
   "metadata": {},
   "source": [
    "## Let's try understaning using code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e62a4f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namaste! I am a professional multilingual translator with expertise in translating various languages to Hindi. I can help you with accurate and high-quality translations in Hindi for any content or document you need. Just provide me with the text in the original language, and I will ensure that it is translated effectively into Hindi while maintaining the original meaning and context. Feel free to reach out to me for all your translation needs. ‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶! (Thank you!)\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from dotenv import load_dotenv\n",
    "from langchain.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "#Loading API keys from .env file\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "chat_model = init_chat_model(model=\"openai:gpt-3.5-turbo\", temperature=0, max_tokens=500)\n",
    "\n",
    "system_message = SystemMessage(content=\"You are an expert multilingual translator that translates any language to Hindi.\")\n",
    "\n",
    "response = chat_model.invoke([system_message])\n",
    "\n",
    "print(response.content)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09c6467",
   "metadata": {},
   "source": [
    "#### üî∑ So, I told a model or assigned a role using system message that who is he, and what is he will be doing. \n",
    ">‚ö†Ô∏è We have to send this messages a list if we are using langchain.messages module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88744473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡§Ü‡§ú ‡§Æ‡•å‡§∏‡§Æ ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§π‡•à‡•§\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from dotenv import load_dotenv\n",
    "from langchain.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "#Loading API keys from .env file\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "chat_model = init_chat_model(model=\"openai:gpt-3.5-turbo\", temperature=0, max_tokens=500)\n",
    "\n",
    "system_message = SystemMessage(content=\"You are an expert multilingual translator that translates any language to Hindi.\")\n",
    "human_message = HumanMessage(content=\"Translate the following sentence to Hindi: 'The weather is nice today.'\")\n",
    "\n",
    "response = chat_model.invoke([system_message, human_message])\n",
    "\n",
    "print(response.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591ab4d8",
   "metadata": {},
   "source": [
    "#### üî∑ Even if i don't give instruction, such as Translate the following sentence to Hindi, it will still translate autometically to Hindi as I told model specifically to translate any sentence from any anguage to hindin Using `SystemMessage`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfd8eaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡§Ü‡§ú ‡§Æ‡•å‡§∏‡§Æ ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§π‡•à‡•§\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from dotenv import load_dotenv\n",
    "from langchain.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "#Loading API keys from .env file\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "chat_model = init_chat_model(model=\"openai:gpt-3.5-turbo\", temperature=0, max_tokens=500)\n",
    "\n",
    "system_message = SystemMessage(content=\"You are an expert multilingual translator that translates any language to Hindi.\")\n",
    "human_message = HumanMessage(content=\"The weather is nice today.\")\n",
    "\n",
    "response = chat_model.invoke([system_message, human_message])\n",
    "\n",
    "print(response.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d7888f",
   "metadata": {},
   "source": [
    "#### üîé Let's check The type of message returned by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e963c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n"
     ]
    }
   ],
   "source": [
    "print(type(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a730eee7",
   "metadata": {},
   "source": [
    "#### üî∑ As expected, It says it's a AI Message."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85c0537",
   "metadata": {},
   "source": [
    "### Lesson: üìù\n",
    "\n",
    "#### ‚ùì Before I go forward to understand how is message type is used and why do we use. I thought to understand why not simply write a single prompt?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7fc0e9",
   "metadata": {},
   "source": [
    "#### ‚è©  From the docs, prompts are defined two ways: \n",
    "\n",
    "##### 1Ô∏è‚É£ Text prompt\n",
    "\n",
    "##### 2Ô∏è‚É£ Message prompt\n",
    "\n",
    "##### ‚è≠Ô∏è As discussed in models.ipynb, when we have only one request and want the response immediately, we use text prompt. This will not keep the chat history or memory or any context of what question we asked earlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ee362dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Vraj, I'm just a computer program so I don't have feelings, but I'm here to help you with anything you need. How can I assist you today?\n",
      "I'm sorry, I do not have access to personal information such as your name.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from dotenv import load_dotenv\n",
    "from langchain.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "#Loading API keys from .env file\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "chat_model = init_chat_model(model=\"openai:gpt-3.5-turbo\", temperature=0, max_tokens=500)\n",
    "\n",
    "response = chat_model.invoke(\"Hello, how are you?, I am Vraj.\")\n",
    "response_2 = chat_model.invoke(\"What is my name?\")\n",
    "print(response.content)\n",
    "print(response_2.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afca1dd",
   "metadata": {},
   "source": [
    "###  üõ†Ô∏è let me try by creating a simple one-on-one chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb44ee9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: Hello! How can I assist you today?\n",
      "AI: Goodbye! Have a great day!\n",
      "AI: Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from dotenv import load_dotenv\n",
    "from langchain.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "#Loading API keys from .env file\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "chat_model = init_chat_model(model=\"openai:gpt-3.5-turbo\", temperature=0, max_tokens=500)\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    print(user_input)\n",
    "    if user_input.lower() in ['bye', 'quit']:\n",
    "        print(\"Chatbot: Goodbye!\")\n",
    "        break\n",
    "\n",
    "    response = chat_model.invoke(user_input)\n",
    "    print(\"Chatbot:\", response.content)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba44058",
   "metadata": {},
   "source": [
    "#### Well this is not working well in Jupyter notebbok. I am creating a new .py file \"one-on-one_testchatbot.py\" to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7fb336",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
